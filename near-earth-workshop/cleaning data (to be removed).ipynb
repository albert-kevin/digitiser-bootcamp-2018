{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"2vr3-k9wn.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all', percentiles=[.05, .25, .5, .75, 0.95])\n",
    "#50 percentile = median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice something wrong:\n",
    "* \"h_mag\" we have only 181/202 continuous values, missing 11\n",
    "* \"period_yr\" has 200/202 values, missing 2\n",
    "* \"q_au_2\" has 200/202, probably 2 values wrong\n",
    "\n",
    "the other columns with categorical data:\n",
    "* designation: how many unique values ? sometimes it holds date data, to be cleaned, 202/202 ok\n",
    "* discovery_date: object, probably string type, needs to be datetime, split into years and month and day columns, 202/202 ok\n",
    "* orbit_class: str 202/202 looks good, perhaps transform into dummy columns for numerical analysis if needed\n",
    "* pha: binary values, str type 'Y' or 'N', maybe replace with binary value N=0 and Y=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a variable\n",
    "h_mag = df[\"h_mag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check all unique values in this column, notice 'nan', could be null or string, must be cleaned\n",
    "#we could choose to lower the data type from float64 to int and lower the storage space\n",
    "h_mag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_mag[ h_mag.isnull() ]\n",
    "#these are all the records with null values, (21x values, 10% that's a lot !)\n",
    "#what we could try is fill in with dummy values based on mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "h_mag.plot.hist(bins=len(h_mag))\n",
    "plt.plot()\n",
    "\n",
    "#this shows some sort of a normal distribution, with a mean=20.3 and a standard deviation=1.52\n",
    "#the range = 24.3 - 15.6 = 8,7\n",
    "#let's check the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(h_mag[h_mag.notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(h_mag[h_mag.notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is GOOD ! this means that we can use 20.3 as a default value to replace the NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_mag[ h_mag.isnull() ] = float(20.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's work on \"period_yr\"\n",
    "period_yr = df[\"period_yr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_yr.unique()\n",
    "# again some nan values\n",
    "# some extremely large numbers, lets check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_yr[ period_yr.isnull() ]\n",
    "#ok so only 2x nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#between Q1 and Q3 : 1.85 and 4.39\n",
    "#so, what are the values inside this ideal IQR range ? \n",
    "period_yr[ (period_yr > 1.855000e+00) & (period_yr < 4.390000e+00) ]\n",
    "#Maybe we need a z-score for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = period_yr[ (period_yr > 9.405000e-01) & (period_yr < 2.354150e+01) ]\n",
    "plt.figure(figsize=(12,8))\n",
    "x.plot.hist(bins=len(x))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quite a uniform distribution, we can replace the 2 nan values with the median value 3.25\n",
    "period_yr[ period_yr.isnull() ] = float(3.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_yr[147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = period_yr[ (period_yr > 0) & (period_yr < 10) ]\n",
    "plt.figure(figsize=(12,8))\n",
    "x.plot.hist(bins=len(x))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(x, bins=40)\n",
    "#I think if we replace the outlier values with these median values that the distribution becomes more normal,\n",
    "#but I am afraid this would damage the truth these numbers are telling, maybe a sigma 2 or 3 cutoff would be better...\n",
    "#don't know how to do or if important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok so now we should try to clean \"q_au_1\"\n",
    "q_au_1 = df[\"q_au_1\"]\n",
    "q_au_2 = df[\"q_au_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_au_1.unique()\n",
    "# looking rather normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_au_2.unique()\n",
    "\n",
    "# this should be cleaned..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show only the values between the 5% and 95% range, erase the outliers\n",
    "x = q_au_2[ (q_au_2 > 1.228000) & (q_au_2 < 16.947000) ]\n",
    "plt.figure(figsize=(12,8))\n",
    "x.plot.hist(bins=len(x))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_au_2[ q_au_2.isnull() ]\n",
    "#ok so only 2x nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quite a uniform distribution, we can replace the 2 nan values with the median value 3.25\n",
    "q_au_2[ q_au_2.isnull() ] = float(3.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leaving the outliers... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation = df[\"designation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation.nunique()\n",
    "# we have 202 unique observations/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation[designation.unique()]\n",
    "# we should clean up this data, remove the weird number, remove the date number, and simply keep the code like \"AH37\"\n",
    "# as a string value, we can try to use regular expressions for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = designation.unique()[0]\n",
    "print (text)\n",
    "#let's exercise cleaning this one thing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#loading regex / regular expression tools\n",
    "#also go to https://regex101.com/  for getting the right code\n",
    "#check out : https://en.wikipedia.org/wiki/Provisional_designation_in_astronomy   to figure out the letter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "example text:\n",
    "\n",
    "419880 (2011 AH37)\n",
    "(2015 HF11)\n",
    "C/2010 E3 (WISE)\n",
    "(2010 CP140)\n",
    "\n",
    "[ABCDEFGHJKLMNOPQRSTUVWXY][ABCDEFGHJKLMNOPQRSTUVWXYZ]?[\\d]+\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"[ABCDEFGHJKLMNOPQRSTUVWXY][ABCDEFGHJKLMNOPQRSTUVWXYZ]?[\\d]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = (\"419880 (2011 AH37)\\n\"\n",
    "            \"(2015 HF11)\\n\"\n",
    "            \"C/2010 E3 (WISE)\\n\"\n",
    "            \"(2010 CP140)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(regex, test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok, so this is proove we can extract this content and now we need to create a function to automate this in the dataset\n",
    "#if we replace the test_str with the \"designation\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = designation.values\n",
    "test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_str in designation:\n",
    "    print (test_str)\n",
    "    print (re.findall(regex, test_str))\n",
    "    #if we verify this list we can see mistakes that we need to resolve\n",
    "    #if empty fill in original value or do another regex translation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_date = df[\"discovery_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_date[0].split(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_date = discovery_date[0].split(\"T\")[0]\n",
    "one_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_date = datetime.strptime(one_date, \"%Y-%m-%d\")\n",
    "one_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (one_date.year)\n",
    "print (one_date.month)\n",
    "print (one_date.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we update the dataframe with the right columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"discovery_date\"] = discovery_date.apply(lambda x: x.split(\"T\")[0])\n",
    "df[\"discovery_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"discovery_year\"] = df[\"discovery_date\"].apply(lambda x: int(x.split(\"-\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"discovery_month\"] = df[\"discovery_date\"].apply(lambda x: int(x.split(\"-\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"discovery_day\"] = df[\"discovery_date\"].apply(lambda x: int(x.split(\"-\")[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['discovery_date'], axis=1, inplace=True)\n",
    "#removing the original date string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#... work in progress, quite fun... could all be worthless  ! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "http://4cminews.com/?p=30879\n",
    "https://en.wikipedia.org/wiki/2016_WF9\n",
    "\n",
    "designation    = year + identifier of the object in space (ex: 2016 WF9) - wikipedia.org/wiki/2016_WF9\n",
    "discovery_date = the date the NEOWISE space RADAR detected the object (discovery: first observed) yyyy-mm-dd\n",
    "h_mag          = absolute magnitude of the object in log scale (ex: H (mag)= 20.1) - wikipedia.org/wiki/Absolute_magnitude\n",
    "               = only objects larger than roughly 140 meters in diameter (or absolute magnitude, H > 22)\n",
    "               = see : https://en.wikipedia.org/wiki/Minimum_orbit_intersection_distance\n",
    "i_deg          = inclination degree of the orbit (irrelevant), the tilt of the object orbit around a body\n",
    "moid_au        = minimum orbit intersection distance\n",
    "               = An object is classified as a potentially hazardous object (PHO) – that is,posing a possible risk to Earth –\n",
    "               = if, among other conditions, its Earth MOID is less than 0.05 AU.\n",
    "               = (MOID < 0.05)\n",
    "               = https://en.wikipedia.org/wiki/Minimum_orbit_intersection_distance\n",
    "               = ex: earth MOID for 2016_WF9 = 0.0156 AU ~ MOID (au): 0.015 in dataset\n",
    "orbit_class    = group name of the Near Earth Orbit route around the sun compared to earth (irrelevant)\n",
    "period_yr      = how many earth years the object makes it's own full orbit\n",
    "               = the time a given astronomical object takes to complete one orbit around another object\n",
    "               = in degrees °, ex:  Inclination\t14.995°  or i (deg): 15 (irrelevant)\n",
    "pha            = potentially hazardous asteroids (Y/N) binary data\n",
    "               = suspected extinct comet, classified as near-Earth object and potentially hazardous asteroid of Apollo group\n",
    "q_au_1         = (AU) min amplitude (= earth is at 1 AU)\n",
    "               = q (au): 0.98 or Perihelion  0.9816 AU\n",
    "               = the point where the body comes closest to the Sun\n",
    "q_au_2         = (AU) max amplitude (= earth is at 1 AU)\n",
    "               = Q (au): 4.76 or Aphelion  4.7614 AU\n",
    "               = which is the point in the orbit where the celestial body is farthest from the Sun\n",
    "               = https://en.wikipedia.org/wiki/Perihelion_and_aphelion\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
